---
title: 基于距离的计算方法
date: 2017-09-17 13:37:46
tags: 算法
mathjax: true
password: 
---

# 基于距离的计算方法

在因为之前项目中用到了欧氏距离，所以在网上看了一下，然后感觉还不错就转载到自己的笔记中。顺便测试了一下在博客中直接编写公式。

本文转自：http://blog.sina.com.cn/s/blog_52510b1d01015nrg.html

<!--more-->

# 欧氏距离(Euclidean Distance)

欧氏距离是最易于理解的一种距离计算方法，源自欧氏空间中两点间的距离公式。

(1)二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：

$$
d_{12}=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}
$$

(2)三维空间两点a(x1,y1,z1)与b(x2,y2,z2)间的欧氏距离：

$$
d_{12}=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2+(z_1-z_2)^2}
$$

(3)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的欧氏距离：

$$
d_{12}=\sqrt{\sum_{k=1}^n(x_{1k}-x_{2k})^2}
$$

也可以用表示成向量运算的形式：

$$
d_{12}=\sqrt{(a-b)(a-b)^T}
$$

(4)Matlab计算欧氏距离

Matlab计算距离主要使用pdist函数。若X是一个M×N的矩阵，则pdist(X)将X矩阵M行的每一行作为一个N维向量，然后计算这M个向量两两间的距离。

例子：计算向量(0,0)、(1,0)、(0,2)两两间的欧式距离

X = [0 0 ; 1 0 ; 0 2]

D = pdist(X,'euclidean')

结果：

D =

    1.0000    2.0000    2.2361

# 曼哈顿距离(Manhattan Distance)

从名字就可以猜出这种距离的计算方法了。想象你在曼哈顿要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源， 曼哈顿距离也称为**城市街区距离(City Block distance)**。

(1)二维平面两点a(x1,y1)与b(x2,y2)间的曼哈顿距离

$$
d_{12}=|x_1-x_2|+|y_1-y_2|
$$

(2)两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的曼哈顿距离

$$
d_{12}=\sum_{k=1}^n|x_{1k}-x_{2k}|
$$

(3) Matlab计算曼哈顿距离

例子：计算向量(0,0)、(1,0)、(0,2)两两间的曼哈顿距离

X = [0 0 ; 1 0 ; 0 2]

D = pdist(X, 'cityblock')

结果：

D =

     1     2     3

# 标准化欧氏距离(Standardized Euclidean distance )

(1)标准欧氏距离的定义

标准化欧氏距离是针对简单欧氏距离的缺点而作的一种改进方案。标准欧氏距离的思路：既然数据各维分量的分布不一样，好吧！那我先将各个分量都“标准化”到均值、方差相等吧。均值和方差标准化到多少呢？这里先复习点统计学知识吧，假设样本集X的均值(mean)为m，标准差(standard deviation)为s，那么X的“标准化变量”表示为：

而且标准化变量的数学期望为0，方差为1。因此样本集的标准化过程(standardization)用公式描述就是：

$$
X^* = \frac{X-m}{s}
$$

标准化后的值 =  ( 标准化前的值  － 分量的均值 ) /分量的标准差

经过简单的推导就可以得到两个n维向量a(x11,x12,…,x1n)与 b(x21,x22,…,x2n)间的标准化欧氏距离的公式：

$$
d_{12}=\sqrt{\sum_{k=1}^n(\frac{x_{1k}-x_{2k}}{s_k})^2}
$$

如果将方差的倒数看成是一个权重，这个公式可以看成是一种**加权欧氏距离(Weighted Euclidean distance)**。

(2)Matlab计算标准化欧氏距离

例子：计算向量(0,0)、(1,0)、(0,2)两两间的标准化欧氏距离 (假设两个分量的标准差分别为0.5和1)

X = [0 0 ; 1 0 ; 0 2]

D = pdist(X, 'seuclidean',[0.5,1])

结果：

D =

    2.0000    2.0000    2.8284

# 夹角余弦(Cosine)

有没有搞错，又不是学几何，怎么扯到夹角余弦了？各位看官稍安勿躁。几何中夹角余弦可用来衡量两个向量方向的差异，机器学习中借用这一概念来衡量样本向量之间的差异。

(1)在二维空间中向量A(x1,y1)与向量B(x2,y2)的夹角余弦公式：

$$
\cos \theta = \frac{x_1x_2+y_1y_2}{\sqrt{x_1^2+y_1^2} \sqrt{x_2^2+y_2^2}}
$$

(2) 两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)的夹角余弦

类似的，对于两个n维样本点a(x11,x12,…,x1n)和b(x21,x22,…,x2n)，可以使用类似于夹角余弦的概念来衡量它们间的相似程度。


$$
\cos(\theta)=\frac{a \cdot b}{|a||b|}
$$

即：

$$
\cos(\theta)=\frac{\sum_{k=1}^n x_{1k}x_{2k}}{\sqrt{\sum_{k=1}^nx_{1k}^2}\sqrt{\sum_{k=1}^nx_{2k}^2}}
$$

夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。

(3)Matlab计算夹角余弦

例子：计算(1,0)、( 1,1.732)、( -1,0)两两间的夹角余弦

X = [1 0 ; 1 1.732 ; -1 0]

D = 1- pdist(X, 'cosine')  % Matlab中的pdist(X, 'cosine')得到的是1减夹角余弦的值

结果：

D =

        0.5000   -1.0000   -0.5000





















